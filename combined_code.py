# -*- coding: utf-8 -*-
"""TCC

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nQV0rm78jVr6kMrgJxw-hyUIZcUXjEPV
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.model_selection import GridSearchCV
from sklearn.svm import SVC
from sklearn.decomposition import PCA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
import math
from sklearn.metrics import confusion_matrix, classification_report

tipo1 = ['Normal','Mass','Aerodynamic']
#tipo1 = ['Normal']
ti1   = ['5.0ti','11.3ti','17.5ti','23.8ti','30.0ti']
#ti1   = ['17.5ti']
vel1  = ['15.0ms','17.3ms','19.5ms','21.8ms','24.0ms']
#vel1  = ['19.5ms']

data1 = np.zeros((119999,1), dtype=np.float64)

for tipo in tipo1: #4 tipos       
    if tipo == 'Mass':
        tipo_esp2 = [' 97 ',' 102 ',' 105 ']

    if tipo == 'Aerodynamic':
        tipo_esp2 = [' 2 graus ',' 3 graus ',' 4 graus ']
        
    if tipo == 'Normal':
        tipo_esp2 = [' ']
            
    for tipo_esp in tipo_esp2: #3 tipos
        print(tipo_esp)
        for vel in vel1: #5 velocidades
            for ti in ti1: #2 turbulencias 

                ia = pd.read_csv(r'/content/drive/My Drive/Universidade/UFSM/TCC/Dados/Banco de dados 30-01-20/Todas/Ia '+str(tipo)+str(tipo_esp)+str(vel)+' '+str(ti)+'.csv',index_col=0, sep=',')
                ia = ia.to_numpy()

                for i in range(0,12):
                    ia_t = np.reshape(ia[:,i],(119999,1))
                    #data1 = np.concatenate((data1,ia_t,ib_t,ic_t), axis=1)
                    data1 = np.concatenate((data1,ia_t), axis=1)
data1 = data1[:,1:]
data = pd.DataFrame(data1)
data.to_csv(r'/content/drive/My Drive/Universidade/UFSM/TCC/Dados/Banco de dados 30-01-20/Ia/Ia Completo.csv')

data = pd.read_csv(r'/content/drive/My Drive/Universidade/UFSM/TCC/Dados/Banco de dados 30-01-20/Ia/Ia completo.csv',index_col=0, sep=',')

####### MALIK TESTE PARA     PCA E LDA


####################################################################################################

data = pd.read_csv(r'/content/drive/My Drive/Universidade/UFSM/TCC/Dados/Banco de dados 30-01-20/Ia/Ia completo.csv',index_col=0, sep=',')
y = np.ones((2100), dtype=np.int32)
y[    : 300] = 100     
y[ 300: 600] = 2
y[ 600: 900] = 3 
y[ 900:1200] = 4
y[1200:1500] = 97     
y[1500:1800] = 102 
y[1800:2100] = 105 

acuracia = np.zeros((6,10), dtype=np.float32)
coluna=0
linha=0
#LDA_var = np.arange(1, 7, 1)
#PCA_var = np.arange(599, 2199, 200)
LDA_var = [6]
PCA_var = [2000]

data_1 = data.transpose()

scaler = MinMaxScaler()  # Default behavior is to scale to [0,1]
x = scaler.fit_transform(data_1)
PCA1 = len(x[0])
print('Tamanho de X: ',PCA1)

for PCA_num in PCA_var:
    
    x_PCA = PCA(n_components=PCA_num).fit_transform(x)
    PCA1 = len(x_PCA[0])
    print('Tamanho de X_PCA: ',PCA1)
    
    for LDA_num in LDA_var:
        
        lda = LinearDiscriminantAnalysis(n_components=LDA_num)
        LDA1 = lda.n_components
        
        x_LDA = lda.fit(x_PCA, y).transform(x_PCA)

        X_train, X_test, y_train, y_test = train_test_split(x_LDA, y, test_size=0.3, random_state=1) #DIVIDE DADOS        

        param_grid = {'C': [0.001, 0.005, 0.01, 0.1,1,10],  
                      'gamma': [0.00001,0.0001,0.001, 0.005, 0.01, 0.1,], 
                      'kernel': ['rbf','poly','linear','sigmoid']} 
        grid = GridSearchCV(SVC(),param_grid,n_jobs=-1, cv=10,verbose=1,return_train_score=True)
        grid = grid.fit(X_train, y_train)
        y_pred = grid.predict(X_test)
        acertos = accuracy_score(y_test, y_pred, normalize=True, sample_weight=None)
        
        acuracia[linha,coluna] = (acertos*100)
        linha=linha+1
    coluna = coluna+1
    linha=0
    
fig, ax = plt.subplots()
ax.plot(PCA_var, acuracia[5], label='6 LDA')
ax.set(xlabel='Componentes de PCA', ylabel='Acurácia',
        title='Acurácia x PCA - TE Malik')
ax.grid()
plt.legend(title='LDAs:')
#fig.savefig(r"/content/drive/My Drive/Universidade/UFSM/TCC/Dados/Banco de dados 30-01-20/Ia/Colaboratory 27 04 20/Acuracia x PCA - TE Malik 6 LDA.png", dpi = 470)

#plt.show()
fig, ax = plt.subplots()
ax.plot(PCA_var, acuracia[0], label='1 LDA')
ax.plot(PCA_var, acuracia[1], label='2 LDA')
ax.plot(PCA_var, acuracia[2], label='3 LDA')
ax.plot(PCA_var, acuracia[3], label='4 LDA')
ax.plot(PCA_var, acuracia[4], label='5 LDA')
ax.plot(PCA_var, acuracia[5], label='6 LDA')
plt.axis([200, 2100, 0, 100])

ax.set(xlabel='Componentes de PCA', ylabel='Acurácia (%)',
        title='Acurácia x PCA - TE Malik')
ax.grid()
plt.legend(title='LDAs:')
#fig.savefig(r"/content/drive/My Drive/Universidade/UFSM/TCC/Dados/Banco de dados 30-01-20/Ia/Colaboratory 27 04 20/Acuracia x PCA - TE Malik All LDA.png", dpi = 470)

####### MALIK TESTE PARA  PCA E LDA FIXO


####################################################################################################
"""
data = pd.read_csv(r'/content/drive/My Drive/Universidade/UFSM/TCC/Dados/Banco de dados 30-01-20/Ia/Ia completo.csv',index_col=0, sep=',')
y = np.ones((2100), dtype=np.int32)
y[    : 300] = 100     
y[ 300: 600] = 2
y[ 600: 900] = 3 
y[ 900:1200] = 4
y[1200:1500] = 97     
y[1500:1800] = 102 
y[1800:2100] = 105 

#LDA_var = np.arange(1, 7, 1)
#PCA_var = np.arange(599, 2199, 200)
LDA_var = [6]
PCA_var = [2000]

data_1 = data.transpose()

scaler = MinMaxScaler()  # Default behavior is to scale to [0,1]
x = scaler.fit_transform(data_1)
PCA1 = len(x[0])
print('Tamanho de X: ',PCA1)
    
x_PCA = PCA(n_components=2000).fit_transform(x)
PCA1 = len(x_PCA[0])
print('Tamanho de X_PCA: ',PCA1)

lda = LinearDiscriminantAnalysis(n_components=6)

x_LDA = lda.fit(x_PCA, y).transform(x_PCA)
i=i+1
"""


X_train, X_test, y_train, y_test = train_test_split(x_LDA, y, test_size=0.3, random_state=6) #DIVIDE DADOS        

param_grid = {'C': [0.1],  
              'gamma': [0.05], 
              'kernel': ['rbf']} 
grid = GridSearchCV(SVC(),param_grid,n_jobs=-1, cv=10,verbose=1,return_train_score=True)
grid = grid.fit(X_train, y_train)
y_pred = grid.predict(X_test)
# print best parameter after tuning 
print(grid.best_params_) 
# print how our model looks after hyper-parameter tuning 
print(grid.best_estimator_)
acertos = accuracy_score(y_test, y_pred, normalize=True, sample_weight=None)
print(classification_report(y_test, y_pred))
acertos

estimator = grid.best_estimator_
C = grid.best_estimator_.C
gamma = grid.best_estimator_.gamma
kernel =  grid.best_estimator_.kernel

#DEFININDO MATRIZ DE CONFUSÃO

def plot_confusion_matrix(y_true, y_pred,classes,
                          normalize=True,
                          title=None,
                          cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    if not title:
        if normalize:
            title = 'Normalized confusion matrix'
        else:
            title = 'Confusion matrix, without normalization'

    # Compute confusion matrix
    cm = confusion_matrix(y_true, y_pred)
    #classes = classes[unique_labels(y_true, y_pred)]
    # Only use the labels that appear in the data
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')

    print(cm)

    fig, ax = plt.subplots()
    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)
    ax.figure.colorbar(im, ax=ax)
    # We want to show all ticks...
    ax.set(xticks=np.arange(cm.shape[1]),
           yticks=np.arange(cm.shape[0]),
           # ... and label them with the respective list entries
           xticklabels=classes, yticklabels=classes,
           title=title,
           ylabel='True label',
           xlabel='Predicted label')

    # Rotate the tick labels and set their alignment.
    plt.setp(ax.get_xticklabels(), rotation=45, ha="right",fontsize=16,
             rotation_mode="anchor")

    # Loop over data dimensions and create text annotations.
    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            ax.text(j, i, format(cm[i, j], fmt),
                    ha="center", va="center", fontsize=16,
                    color="white" if cm[i, j] > thresh else "black")
    fig.tight_layout()
    return ax

#FIM DA DEFINIÇÃO DA MATRIZ DE CONFUSAO

class_names = ['100% Mass','2 graus','3 graus','4 graus','97% Mass','102% Mass','105% Mass']
plot_confusion_matrix(y_test, y_pred,classes = class_names,title='Confusion matrix')
t1 = ("Kernel =  "+str(kernel))
t2 = ("C =  "+str(C))
t3 = ("Gamma =  "+str(gamma))
#t4 = ("Teste =  "+str(T_test))
plt.text(6.5, 7, t1, fontsize=11, style='oblique', ha='left', va='top', wrap=True)
plt.text(6.5, 7.2, t2, fontsize=11, style='oblique', ha='left', va='top', wrap=True)
plt.text(6.5, 7.4, t3, fontsize=11, style='oblique', ha='left', va='top', wrap=True)
#plt.text(6.5, 7.6, t4, fontsize=11, style='oblique', ha='left', va='top', wrap=True)
plt.rcParams['figure.figsize'] = (9,7)
#plt.tight_layout()
plt.tick_params(axis = 'y', labelsize = 11)
plt.tick_params(axis = 'x', labelsize = 11)

class_names = ['2 graus','3 graus','4 graus','97% Mass','100% Mass','102% Mass','105% Mass']
plot_confusion_matrix(y_test, y_pred,classes = class_names,title='Confusion matrix')
#t1 = ("Kernel =  "+str(kernel))
#t2 = ("C =  "+str(C))
#t3 = ("Gamma =  "+str(gamma))
#t4 = ("Teste =  "+str(T_test))
t4 = ("Acuracy =  "+str(acertos*100))
#plt.text(6.5, 7, t1, fontsize=11, style='oblique', ha='left', va='top', wrap=True)
#plt.text(6.5, 7.2, t2, fontsize=11, style='oblique', ha='left', va='top', wrap=True)
#plt.text(6.5, 7.4, t3, fontsize=11, style='oblique', ha='left', va='top', wrap=True)
plt.text(6.5, 7.6, t4, fontsize=11, style='oblique', ha='left', va='top', wrap=True)
plt.rcParams['figure.figsize'] = (9,7)
#plt.tight_layout()
plt.tick_params(axis = 'y', labelsize = 11)
plt.tick_params(axis = 'x', labelsize = 11)
plt.savefig(r"/content/drive/My Drive/Universidade/UFSM/TCC/Dados/Banco de dados 30-01-20/Ia/Colaboratory 21-02-21/Confusion Matrix "+str(i)+".png", dpi = 470)
#fig.savefig(r"/content/drive/My Drive/Universidade/UFSM/TCC/Dados/Banco de dados 30-01-20/Ia Ib Ic/stats/Kandukuri"+str(i)+".png", dpi = 470)

param_grid = {'C': [0.1,0.1,0.1,0.1,1,10,10000],  
              'gamma': [0.001, 0.005, 0.01, 0.1,1,10], 
              'kernel': ['rbf','poly','linear','sigmoid']} 
grid = GridSearchCV(SVC(),param_grid,n_jobs=-1, cv=10,verbose=1,return_train_score=True)
grid = grid.fit(X_train, y_train)
y_pred = grid.predict(X_test)
from sklearn.metrics import confusion_matrix, classification_report
# print best parameter after tuning 
print(grid.best_params_) 
# print how our model looks after hyper-parameter tuning 
print(grid.best_estimator_)
acertos = accuracy_score(y_test, y_pred, normalize=True, sample_weight=None)
print(classification_report(y_test, y_pred))
acertos

####### MALIK TESTE PARA     PCA E *SEM* LDA

#############################################################################################
data = pd.read_csv(r'/content/drive/My Drive/Universidade/UFSM/TCC/Dados/Banco de dados 30-01-20/Ia/Ia completo.csv',index_col=0, sep=',')
           
y = np.ones((2100), dtype=np.int32)
y[    : 300] = 100     
y[ 300: 600] = 2
y[ 600: 900] = 3 
y[ 900:1200] = 4
y[1200:1500] = 97     
y[1500:1800] = 102 
y[1800:2100] = 105 

acuracia = np.zeros((2,5), dtype=np.float32)
coluna=0
linha=0

PCA_var = np.arange(399, 2399, 400)

data_1 = data.transpose()

scaler = MinMaxScaler()  # Default behavior is to scale to [0,1]
x = scaler.fit_transform(data_1)
PCA1 = len(x[0])
print('Tamanho de X: ',PCA1)

for PCA_num in PCA_var:
    
    x_PCA = PCA(n_components=PCA_num).fit_transform(x)
    PCA1 = len(x_PCA[0])
    print('Tamanho de X_PCA: ',PCA1)

    X_train, X_test, y_train, y_test = train_test_split(x_PCA, y, test_size=0.3, random_state=1) #DIVIDE DADOS        

    param_grid = {'C': [0.005, 0.01, 0.1,1,10],  
                  'gamma': [0.0001,0.001, 0.005], 
                  'kernel': ['rbf','poly','sigmoid']} 
    grid = GridSearchCV(SVC(),param_grid,n_jobs=-1, cv=3,verbose=1,return_train_score=True)
    grid = grid.fit(X_train, y_train)
    y_pred = grid.predict(X_test)
    acertos = accuracy_score(y_test, y_pred, normalize=True, sample_weight=None)

    acuracia[linha,coluna] = (acertos*100)

    linha=linha+1
    
    #LDA    
    lda = LinearDiscriminantAnalysis(n_components=6)
    LDA1 = lda.n_components
    
    x_LDA = lda.fit(x_PCA, y).transform(x_PCA)

    X_train, X_test, y_train, y_test = train_test_split(x_LDA, y, test_size=0.3, random_state=1) #DIVIDE DADOS        

    param_grid = {'C': [0.005, 0.01, 0.1,1,10],  
                  'gamma': [0.0001,0.001, 0.005], 
                  'kernel': ['rbf','poly','sigmoid']} 
    grid = GridSearchCV(SVC(),param_grid,n_jobs=-1, cv=3,verbose=1,return_train_score=True)
    grid = grid.fit(X_train, y_train)
    y_pred = grid.predict(X_test)
    acertos = accuracy_score(y_test, y_pred, normalize=True, sample_weight=None)
    
    acuracia[linha,coluna] = (acertos*100)


    linha=0
    coluna = coluna+1
    

#plt.show()
fig, ax = plt.subplots()
ax.plot(PCA_var, acuracia[0], label='Somente PCA')
ax.plot(PCA_var, acuracia[1], label='PCA + LDA')

plt.axis([200, 2100, 0, 100])

ax.set(xlabel='Componentes de PCA', ylabel='Acurácia (%)',
        title='LDA x PCA - TE Malik')
ax.grid()
plt.legend(title='Acurácia:')
fig.savefig(r"/content/drive/My Drive/Universidade/UFSM/TCC/Dados/Banco de dados 30-01-20/Ia/Colaboratory 27 04 20/LDA x PCA - TE Malik 2.png", dpi = 470)

####### MALIK TESTE PARA TURBULÊNCIA E VELOCIDADES

numero=20
####################################################################################################
#tipo1 = ['5TI','17.5TI','30TI','15ms','19.5ms','24ms']
tipo1 = ['5TI','17.5TI','30TI']
c = len(tipo1)
coluna=0
linha=0
t = np.arange(0, numero, 1)
acuracia = np.zeros((c,numero), dtype=np.float32) 
i=80  
t2 = np.arange(i, (i+numero), 1)
for tipo in tipo1:
    
    print(tipo)
    x = pd.read_csv(r'/content/drive/My Drive/Universidade/UFSM/TCC/Dados/Banco de dados 30-01-20/Ia/Ia '+str(tipo)+'.csv',index_col=0, sep=',').T

    y = np.ones((420), dtype=np.int32)
    y[   : 60] = 100     
    y[ 60:120] = 97
    y[120:180] = 102 
    y[180:240] = 105
    y[240:300] = 2    
    y[300:360] = 3 
    y[360:420] = 4

    scaler = MinMaxScaler()  # Default behavior is to scale to [0,1]
    x = scaler.fit_transform(x)
    #PCA1 = len(x[0])
    #print('Tamanho de X: ',PCA1)

    x = PCA(400).fit_transform(x)
    PCA1 = len(x[0])
    #print('Tamanho de X_PCA: ',PCA1)
              
    lda = LinearDiscriminantAnalysis(n_components=6)
    LDA1 = lda.n_components
    
    x_LDA = lda.fit(x, y).transform(x)

    for i in t2:        

        X_train, X_test, y_train, y_test = train_test_split(x_LDA, y, test_size=0.3, random_state=i) #DIVIDE DADOS
  
        param_grid = {'C': [0.001, 0.005, 0.01, 0.1,1,10],  
                      'gamma': [0.00001,0.0001,0.001, 0.005, 0.01, 0.1,], 
                      'kernel': ['rbf','poly','linear','sigmoid']} 
        grid = GridSearchCV(SVC(),param_grid,n_jobs=-1, cv=10,verbose=0,return_train_score=True)
 
        grid = grid.fit(X_train, y_train)
 
        y_pred = grid.predict(X_test)

        acertos = accuracy_score(y_test, y_pred, normalize=True, sample_weight=None)

        acuracia[linha,coluna] = (acertos*100)
        coluna = coluna+1
        del X_train, X_test, y_train, y_test
 
    linha=linha+1
    coluna=0
    #i=20
           
#['5TI','17.5TI','30TI','15ms','19.5ms','24ms']
fig, ax = plt.subplots()
b=0
for nome in tipo1:
  ax.plot(t, acuracia[b], label=nome)
  b=b+1

ax.set(xlabel='Simulação', ylabel='Acurácia (%)')
ax.grid()
plt.axis([-0.5, (numero-0.5), 50, 100])
plt.legend(title='Agente:')
fig.savefig(r"/content/drive/My Drive/Universidade/UFSM/TCC/Dados/Banco de dados 30-01-20/Ia/Colaboratory 27 04 20/Impacto agentes externos "+str(contadork)+".png", dpi = 470)
contadork=contadork+1

# PRINTAR IA ALEATÓRIAS

#############################################################################################
#['5TI','17.5TI','30TI','15ms','19.5ms','24ms']
#tipo = 'Normal 15.0ms 5.0ti'
tipo = 'Normal'

linha=0

print(tipo)
#x = pd.read_csv(r'/content/drive/My Drive/Universidade/UFSM/TCC/Dados/Banco de dados 30-01-20/Todas/Ia '+str(tipo)+'.csv',index_col=0, sep=',').T
x = pd.read_csv(r'/content/drive/My Drive/Universidade/UFSM/TCC/Dados/Banco de dados 30-01-20/Ia/Ia '+str(tipo)+'.csv',index_col=0, sep=',').T
x

data = pd.read_csv(r'/content/drive/My Drive/Universidade/UFSM/TCC/Dados/Banco de dados 30-01-20/Ia/Ia completo.csv',index_col=0, sep=',').T
           
y = np.ones((2100), dtype=np.int32)
y[    : 300] = 100     
y[ 300: 600] = 2
y[ 600: 900] = 3 
y[ 900:1200] = 4
y[1200:1500] = 97     
y[1500:1800] = 102 
y[1800:2100] = 105 

scaler = MinMaxScaler()  # Default behavior is to scale to [0,1]
x_scaled = scaler.fit_transform(data)

x_PCA = PCA(2000).fit_transform(x_scaled)
PCA1 = len(x_PCA[0])
print('Tamanho de X_PCA: ',PCA1)
   
lda = LinearDiscriminantAnalysis(n_components=6)
LDA1 = lda.n_components

x_LDA = lda.fit(x_PCA, y).transform(x_PCA)
x_LDA
linha=0

fig, ax = plt.subplots()
t = np.arange(1, (LDA1+1), 1)
ax.plot(t, x_LDA[0], label='Ia LDA')
linha=linha+1

#plt.axis([200, 2100, 0, 100])

ax.set(xlabel='Componentes de LDA')
ax.grid()
plt.legend(title='Ia')
fig.savefig(r"/content/drive/My Drive/Universidade/UFSM/TCC/Dados/Banco de dados 30-01-20/Ia/Colaboratory 27 04 20/Ia LDA "+str(linha)+".png", dpi = 470)

c=50

#x1 = x.iloc[linha:(linha+10),:c].to_numpy()
x1 = x_LDA[linha,:]
t = np.arange(0, (c*0.0005), 0.0005)
fig, ax = plt.subplots()
for i in range(1):
  ax.plot(t, x1[i,:], label='Corrente Ia')
#linha=linha-1
print(linha)
ax.set(xlabel='Tempo (s)', ylabel='Corrente Ia (A)')
ax.grid()
#plt.axis([-0.5, 5, -1000, 1000])
plt.legend(title='Tipo:')
#fig.savefig(r"/content/drive/My Drive/Universidade/UFSM/TCC/Dados/Banco de dados 30-01-20/Ia/Colaboratory 27 04 20/Ia "+str(tipo)+" "+str(linha)+".png", dpi = 470)

# DADOS SOBRE AS PCA

#############################################################################################
data = pd.read_csv(r'/content/drive/My Drive/Universidade/UFSM/TCC/Dados/Banco de dados 30-01-20/Ia/Ia 5TI.csv',index_col=0, sep=',').T

scaler = MinMaxScaler()  # Default behavior is to scale to [0,1]
x = scaler.fit_transform(data)
PCA1 = len(x[0])
print('Tamanho de X: ',PCA1)


#x_PCA = PCA(0.999999).fit_transform(x)
#PCA1 = len(x_PCA[0])
#data.to_csv(r'/content/drive/My Drive/Universidade/UFSM/TCC/Dados/Banco de dados 30-01-20/Ia/Colaboratory 27 04 20/plot PCA/Ia PCA '+str(PCA1)+'.csv')

scaler = MinMaxScaler()  # Default behavior is to scale to [0,1]
x_scaled = scaler.fit_transform(data)
x = x_scaled
PCA1 = len(x[0])
print('Tamanho de X: ',PCA1)

x = x_scaled

pca = PCA(400)
pca.fit(x)
#print(pca.explained_variance_ratio_)
#print(pca.components_)
#print(pca.explained_variance_)
#print(pca.explained_variance_ratio_)
#print(pca.singular_values_)
#print(pca.mean_)
#print(pca.noise_variance_)

#Componentes = PCA.components_
#Variancia = PCA.explained_variance_
#VarianciaNormalizada = PCA.explained_variance_ratio_
#sing_val = PCA.singular_values_
#mean = PCA.mean_
#noise = PCA.noise_variance_

print(pca.components_)

variavel = (pca.components_[1])

t = np.arange(0, len(variavel), 1)

fig, ax = plt.subplots()
ax.plot(t, variavel, label= 'Principal Componente 1 da PCA')

ax.set(xlabel='Características da PCA', ylabel='Amplitude',
      title='Principal Componente da PCA')
ax.grid()
plt.legend(title='PCA:')
#plt.axis([-50, 1050, 0, 100])
fig.savefig(r"/content/drive/My Drive/Universidade/UFSM/TCC/Dados/Banco de dados 30-01-20/Ia/Colaboratory 27 04 20/plot PCA/Variância"+str(i)+".png", dpi = 470)
i=i+1

#### KANDUKURI IP IMPACTO DOS SEGUNDOS

#############################################################################################

#x1 = pd.read_csv(r'/content/drive/My Drive/Universidade/UFSM/TCC/Dados/Ia Ib Ic/IP 5s Resumido com stats 2.csv',index_col=0, sep=',').T
x1 = pd.read_csv(r'/content/drive/My Drive/Universidade/UFSM/TCC/Dados/Banco de dados 30-01-20/Ia Ib Ic/IP 5s Resumido.csv',index_col=0, sep=',').T


y = np.ones((2100), dtype=np.int32)
y[   :300] = 100      #100
y[300:600] = 2     #
y[600:900] = 3     #
y[900:1200] = 4        #
y[1200:1500] = 97      #100
y[1500:1800] = 102     #
y[1800:] = 105
coluna=0
scaler = MinMaxScaler()  # Default behavior is to scale to [0,1]
x1 = scaler.fit_transform(x1)

for i in range(0,10):        
        print(' Coluna:', coluna)
        X_train, X_test, y_train, y_test = train_test_split(x1, y, test_size=0.3, random_state=coluna) #DIVIDE DADOS        

        param_grid = {'C': [ 0.005, 0.01, 0.1,1,10,100,1000],  
                      'gamma': [0.00001,0.0001,0.001, 0.005, 0.01, 0.1,], 
                      'kernel': ['rbf','poly','linear']} 
        grid = GridSearchCV(SVC(),param_grid,n_jobs=-1, cv=10,verbose=1,return_train_score=True)
        grid = grid.fit(X_train, y_train)
        y_pred = grid.predict(X_test)
        acertos = accuracy_score(y_test, y_pred, normalize=True, sample_weight=None)
        C[coluna] = clf.best_estimator_.C
        gamma[coluna] = clf.best_estimator_.gamma
        kernel[coluna] = clf.best_estimator_.kernel
        acuracia[coluna] = (acertos*100)
        coluna = coluna+1
acuracia
C
gamma
kernel

#linha=1
for linha in range(0,100):
  X_train, X_test, y_train, y_test = train_test_split(x_LDA, y, test_size=0.3, random_state=linha)
  #linha=linha+1
  param_grid = {'C': [1,1,1,10,100,5000,10000],  
                'gamma': [0.001, 0.005, 0.01, 0.1,1,10], 
                'kernel': ['rbf','poly','linear','sigmoid']} 
  grid = GridSearchCV(SVC(),param_grid,n_jobs=-1, cv=10,verbose=0,return_train_score=True)
  grid = grid.fit(X_train, y_train)
  y_pred = grid.predict(X_test)
  acertos = accuracy_score(y_test, y_pred, normalize=True, sample_weight=None)
  print(linha)
  print(acertos)
  if acertos > 0.99:

    from sklearn.metrics import confusion_matrix, classification_report
    # print best parameter after tuning 
    print(grid.best_params_) 
    # print how our model looks after hyper-parameter tuning 
    print(grid.best_estimator_)
    acertos = accuracy_score(y_test, y_pred, normalize=True, sample_weight=None)
    print(classification_report(y_test, y_pred))
    break

#### KANDUKURI IP IMPACTO DAS ESTATÍSTICAS

#############################################################################################

x1 = pd.read_csv(r'/content/drive/My Drive/Universidade/UFSM/TCC/Dados/Banco de dados 30-01-20/Ia Ib Ic/IP 5s Resumido com stats 2.csv',index_col=0, sep=',')
x2 = pd.read_csv(r'/content/drive/My Drive/Universidade/UFSM/TCC/Dados/Banco de dados 30-01-20/Ia Ib Ic/IP 5s Resumido.csv',index_col=0, sep=',').T

y = np.ones((2100), dtype=np.int32)
y[   :300] = 100      #100
y[300:600] = 2     #
y[600:900] = 3     #
y[900:1200] = 4        #
y[1200:1500] = 97      #100
y[1500:1800] = 102     #
y[1800:2100] = 105

numero = 10

scaler = MinMaxScaler()  # Default behavior is to scale to [0,1]
x1 = scaler.fit_transform(x1)
x2 = scaler.fit_transform(x2)
linha=0
coluna=0
t = np.arange(0, numero, 1)
acuracia = np.zeros((2,numero), dtype=np.float32) 

for i in range(0,numero):        
        print('Linha:', linha,' Coluna:', coluna)
        X_train, X_test, y_train, y_test = train_test_split(x1, y, test_size=0.3, random_state=coluna) #DIVIDE DADOS        

        param_grid = {'C': [0.01,1,10],  
                      'gamma': [0.0001,0.001], 
                      'kernel': ['rbf']} 
        grid = GridSearchCV(SVC(),param_grid,n_jobs=-1, cv=2,verbose=1,return_train_score=True)
        grid = grid.fit(X_train, y_train)
        y_pred = grid.predict(X_test)
        acertos = accuracy_score(y_test, y_pred, normalize=True, sample_weight=None)
        
        acuracia[linha,coluna] = (acertos*100)

        linha=linha+1
        print('Linha:', linha,' Coluna:', coluna)
        X_train, X_test, y_train, y_test = train_test_split(x2, y, test_size=0.3, random_state=coluna) #DIVIDE DADOS        

        param_grid = {'C': [0.01, 1,10],  
                      'gamma': [0.0001,0.001], 
                      'kernel': ['rbf']} 
        grid = GridSearchCV(SVC(),param_grid,n_jobs=-1, cv=2,verbose=1,return_train_score=True)
        grid = grid.fit(X_train, y_train)
        y_pred = grid.predict(X_test)
        acertos = accuracy_score(y_test, y_pred, normalize=True, sample_weight=None)
        
        acuracia[linha,coluna] = (acertos*100)

        coluna = coluna+1
        linha=0

fig, ax = plt.subplots()
ax.plot(t, acuracia[0], label='IP 5s com estatísticas')
ax.plot(t, acuracia[1], label='IP 5s')

ax.set(xlabel='Simulação', ylabel='Acurácia (%)',
      title='Impacto dos estatísticas na acurácia')
ax.grid()
plt.legend(title='IPs:')
fig.savefig(r"/content/drive/My Drive/Universidade/UFSM/TCC/Dados/Banco de dados 30-01-20/Ia Ib Ic/stats/Impacto stats 2.png", dpi = 470)

fig, ax = plt.subplots()
ax.plot(t, acuracia[0], label='IP 5s com estatísticas')
ax.plot(t, acuracia[1], label='IP 5s')

ax.set(xlabel='Simulação', ylabel='Acurácia (%)',
      title='Impacto dos estatísticas na acurácia')
ax.grid()
plt.axis([-0.3,9.3, 90, 101])
plt.legend(title='IPs:')
fig.savefig(r"/content/drive/My Drive/Universidade/UFSM/TCC/Dados/Banco de dados 30-01-20/Ia Ib Ic/stats/Impacto stats.png", dpi = 470)

#### KANDUKURI IP 

#############################################################################################
x = pd.read_csv(r'/content/drive/My Drive/Universidade/UFSM/TCC/Dados/Banco de dados 30-01-20/Ia Ib Ic/IP 5s Resumido.csv',index_col=0, sep=',').T

y = np.ones((2100), dtype=np.int32)
y[   :300]   = 100      
y[300:600]   = 2     
y[600:900]   = 3    
y[900:1200]  = 4       
y[1200:1500] = 97      
y[1500:1800] = 102     
y[1800:]     = 105

numero = 10                #numero de iterações e papapa
coluna=0
acuracia = np.zeros((numero), dtype=np.float32)
a=0.00001
b=10000
C = np.linspace(a, b, num=10)

scaler = MinMaxScaler()  # Default behavior is to scale to [0,1]
x = scaler.fit_transform(x)

for i in range(0,numero):        
        print(' Coluna:', i)
        X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=i) #DIVIDE DADOS        

        #param_grid = {'C': [ 0.005, 0.01, 0.1,1,10,100,1000],  
                      #'gamma': [0.00001,0.0001,0.001, 0.005, 0.01, 0.1,], 
                      #'kernel': ['rbf','poly','linear']}
        param_grid = {'C': [ 0.005, 0.01, 0.1,1,10,100,1000],  
                      'gamma': [0.00001,0.0001,0.001, 0.005, 0.01, 0.1,], 
                      'kernel': ['rbf','poly','linear']}  
        grid = GridSearchCV(SVC(),param_grid,n_jobs=-1, cv=10,verbose=1,return_train_score=True)
        grid = grid.fit(X_train, y_train)
        y_pred = grid.predict(X_test)
        acertos = accuracy_score(y_test, y_pred, normalize=True, sample_weight=None)

        C[i] = clf.best_estimator_.C
        gamma[i] = clf.best_estimator_.gamma
        kernel[i] = clf.best_estimator_.kernel
        acuracia[i] = (acertos*100)

acuracia
C
gamma
kernel

#### KANDUKURI IP 

#############################################################################################
x = pd.read_csv(r'/content/drive/My Drive/Universidade/UFSM/TCC/Dados/Banco de dados 30-01-20/Ia Ib Ic/Sid/IP 5s Resumido com stats.csv',index_col=0, sep=',').T

y = np.ones((2100), dtype=np.int32)
y[   :300]   = 100      
y[300:600]   = 2     
y[600:900]   = 3    
y[900:1200]  = 4       
y[1200:1500] = 97      
y[1500:1800] = 102     
y[1800:]     = 105

scaler = MinMaxScaler()  # Default behavior is to scale to [0,1]
x = scaler.fit_transform(x)
      
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=i) #DIVIDE DADOS        

param_grid = {'C': [ 0.005, 0.01, 0.1],  
              'gamma': [0.00001,0.0001,0.001, 0.005, 0.01, 0.1], 
              'kernel': ['rbf','poly','linear']} 

grid = GridSearchCV(SVC(),param_grid,n_jobs=-1, cv=5,verbose=1,return_train_score=True)
grid = grid.fit(X_train, y_train)
y_pred = grid.predict(X_test)
acertos = accuracy_score(y_test, y_pred, normalize=True, sample_weight=None)

from sklearn.metrics import confusion_matrix, classification_report
# print best parameter after tuning 
print(grid.best_params_) 
# print how our model looks after hyper-parameter tuning 
print(grid.best_estimator_)
acertos = accuracy_score(y_test, y_pred, normalize=True, sample_weight=None)
print(classification_report(y_test, y_pred))

estimator = grid.best_estimator_
C = grid.best_estimator_.C
gamma = grid.best_estimator_.gamma
kernel =  grid.best_estimator_.kernel

#DEFININDO MATRIZ DE CONFUSÃO

def plot_confusion_matrix(y_true, y_pred,classes,
                          normalize=True,
                          title=None,
                          cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    if not title:
        if normalize:
            title = 'Normalized confusion matrix'
        else:
            title = 'Confusion matrix, without normalization'

    # Compute confusion matrix
    cm = confusion_matrix(y_true, y_pred)
    #classes = classes[unique_labels(y_true, y_pred)]
    # Only use the labels that appear in the data
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')

    print(cm)

    fig, ax = plt.subplots()
    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)
    ax.figure.colorbar(im, ax=ax)
    # We want to show all ticks...
    ax.set(xticks=np.arange(cm.shape[1]),
           yticks=np.arange(cm.shape[0]),
           # ... and label them with the respective list entries
           xticklabels=classes, yticklabels=classes,
           title=title,
           ylabel='True label',
           xlabel='Predicted label')

    # Rotate the tick labels and set their alignment.
    plt.setp(ax.get_xticklabels(), rotation=45, ha="right",fontsize=16,
             rotation_mode="anchor")

    # Loop over data dimensions and create text annotations.
    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            ax.text(j, i, format(cm[i, j], fmt),
                    ha="center", va="center", fontsize=16,
                    color="white" if cm[i, j] > thresh else "black")
    fig.tight_layout()
    return ax

#FIM DA DEFINIÇÃO DA MATRIZ DE CONFUSAO

class_names = ['100% Mass','2 graus','3 graus','4 graus','97% Mass','102% Mass','105% Mass']
plot_confusion_matrix(y_test, y_pred,classes = class_names,title='Confusion matrix')
t1 = ("Kernel =  "+str(kernel))
t2 = ("C =  "+str(C))
t3 = ("Gamma =  "+str(gamma))
#t4 = ("Teste =  "+str(T_test))
plt.text(6.5, 7, t1, fontsize=11, style='oblique', ha='left', va='top', wrap=True)
plt.text(6.5, 7.2, t2, fontsize=11, style='oblique', ha='left', va='top', wrap=True)
plt.text(6.5, 7.4, t3, fontsize=11, style='oblique', ha='left', va='top', wrap=True)
#plt.text(6.5, 7.6, t4, fontsize=11, style='oblique', ha='left', va='top', wrap=True)
plt.rcParams['figure.figsize'] = (9,7)
#plt.tight_layout()
plt.tick_params(axis = 'y', labelsize = 11)
plt.tick_params(axis = 'x', labelsize = 11)

class_names = ['2 graus','3 graus','4 graus','97% Mass','100% Mass','102% Mass','105% Mass']
plot_confusion_matrix(y_test, y_pred,classes = class_names,title='Confusion matrix')
t1 = ("Kernel =  "+str(kernel))
t2 = ("C =  "+str(C))
t3 = ("Gamma =  "+str(gamma))
#t4 = ("Teste =  "+str(T_test))
plt.text(6.5, 7, t1, fontsize=11, style='oblique', ha='left', va='top', wrap=True)
plt.text(6.5, 7.2, t2, fontsize=11, style='oblique', ha='left', va='top', wrap=True)
plt.text(6.5, 7.4, t3, fontsize=11, style='oblique', ha='left', va='top', wrap=True)
#plt.text(6.5, 7.6, t4, fontsize=11, style='oblique', ha='left', va='top', wrap=True)
plt.rcParams['figure.figsize'] = (9,7)
#plt.tight_layout()
plt.tick_params(axis = 'y', labelsize = 11)
plt.tick_params(axis = 'x', labelsize = 11)
plt.savefig(r"/content/drive/My Drive/Universidade/UFSM/TCC/Dados/Banco de dados 30-01-20/Ia Ib Ic/Sid/Kandukuri"+str(i)+".png", dpi = 470)
#fig.savefig(r"/content/drive/My Drive/Universidade/UFSM/TCC/Dados/Banco de dados 30-01-20/Ia Ib Ic/stats/Kandukuri"+str(i)+".png", dpi = 470)
i=i+1

i=1

x = pd.read_csv(r'/content/drive/My Drive/Universidade/UFSM/TCC/Dados/Banco de dados 30-01-20/Ia/Ia completo.csv',index_col=0, sep=',').T

y = np.ones((2100), dtype=np.int32)
y[    : 300] = 100     
y[ 300: 600] = 2
y[ 600: 900] = 3 
y[ 900:1200] = 4
y[1200:1500] = 97     
y[1500:1800] = 102 
y[1800:2100] = 105

scaler = MinMaxScaler()  # Default behavior is to scale to [0,1]
x_norm = scaler.fit_transform(x)
PCA1 = len(x_norm[0])
print('Tamanho de X: ',PCA1)
  
x_PCA = PCA(n_components=0.9999).fit_transform(x_norm)
PCA1 = len(x_PCA[0])
print('Tamanho de X_PCA: ',PCA1)

lda = LinearDiscriminantAnalysis(n_components=6)
LDA1 = lda.n_components
x_LDA = lda.fit(x_PCA, y).transform(x_PCA)

data = pd.DataFrame(x_PCA)
data.to_csv(r'/content/drive/My Drive/Universidade/UFSM/TCC/Dados/Banco de dados 30-01-20/Ia/Ia Completo PCA 99,99%.csv')
del data

#ARTIGO

import numpy as np
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import make_moons, make_circles, make_classification
from sklearn.neural_network import MLPClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.gaussian_process import GaussianProcessClassifier
from sklearn.gaussian_process.kernels import RBF
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis


names = ["Nearest Neighbors", "Linear SVM", "RBF SVM", 
         "Decision Tree", "Random Forest", "Neural Net", "AdaBoost",
         "Naive Bayes", "QDA"]

classifiers = [
    KNeighborsClassifier(3),
    SVC(kernel="linear", C=0.025),
    SVC(gamma=0.005, C=0.1),
    #GaussianProcessClassifier(1.0 * RBF(1.0)),
    DecisionTreeClassifier(max_depth=5),
    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),
    MLPClassifier(alpha=1, max_iter=1000),
    AdaBoostClassifier(),
    GaussianNB(),
    QuadraticDiscriminantAnalysis()]

#X_train, X_test, y_train, y_test = train_test_split(x_LDA, y, test_size=0.3, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(x_LDA, y, test_size=0.3)

# iterate over classifiers
for name, clf in zip(names, classifiers):
    clf.fit(X_train, y_train)
    score = clf.score(X_test, y_test)
    print("Name: "+name+" Score: "+str(score))